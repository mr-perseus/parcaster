{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bp7xn6t5qL_"
   },
   "source": [
    "\n",
    "# Hyperparameter Sweeps for Parking Data in St. Gallen with LSTM\n",
    "\n",
    "In this project, we use Hyperparemter sweeps with Pytorch on \"Weights & Biases\". For further details, check out this [Colab](http://wandb.me/sweeps-colab).\n",
    "\n",
    "A lot of inspirations come from [this GitHub repository](https://github.com/SheezaShabbir/Time-series-Analysis-using-LSTM-RNN-and-GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from models import LSTMModel, RNNModel, GRUModel\n",
    "from scaler import Scaler\n",
    "from data.metadata.metadata import parking_data_labels\n",
    "from data.preprocessing.preprocess_features import PreprocessFeatures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjckmLcx5qL_"
   },
   "source": [
    "## Login to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiEyBrQm5qMB"
   },
   "source": [
    "## Defining the sweep config\n",
    "\n",
    "We define the sweep config via dict in our Jupyter notebook. You can find more information on sweeps in the [documentation](https://docs.wandb.com/sweeps/configuration).\n",
    "\n",
    "You can find a list of all configuration options [here](https://docs.wandb.com/library/sweeps/configuration) and a big collection of examples in YAML format [here](https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion).\n",
    "\n",
    "We use some information for good values from this [source](https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilZtMXIA5qMC"
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'goal': 'minimize',\n",
    "        'name': 'loss (test)'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'model': {\n",
    "            'value': 'lstm'\n",
    "        },\n",
    "        'X_scaler': {\n",
    "            'values': ['standard', 'minmax', 'robust', 'maxabs']\n",
    "        },\n",
    "        'y_scaler': {\n",
    "            'values': ['standard', 'minmax', 'robust', 'maxabs']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'max': 256,\n",
    "            'min': 32,\n",
    "            'q': 8\n",
    "        },\n",
    "        'train_val_ratio': {\n",
    "            'value': 0.8\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.1, 0.2, 0.5]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [2, 4, 8, 16, 32]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [5, 10, 20, 40]\n",
    "        },\n",
    "        'fc_layer_size': {\n",
    "            'values': [50, 100, 200, 400, 1000]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.1,\n",
    "            'min': 0.00001\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHesSoz85qMF"
   },
   "source": [
    "## Initialize the setup\n",
    "\n",
    "You can find the runs [here](https://wandb.ai/parcaster/pp-sg-lstm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PbIZ_xm5qMG"
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"pp-sg-lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQH5jXxb5qMG"
   },
   "source": [
    "## Run the sweep agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkRbcMX35qMG"
   },
   "source": [
    "### Define Your Training Procedure\n",
    "\n",
    "Before we can actually execute the sweep, we need to define the training procedure that uses those values.\n",
    "\n",
    "In the functions below, we define a simple fully-connected neural network in PyTorch, and add the following `wandb` tools to log model metrics, visualize performance and output and track our experiments:\n",
    "* [**`wandb.init()`**](https://docs.wandb.com/library/init) – Initialize a new W&B Run. Each Run is a single execution of the training function.\n",
    "* [**`wandb.config`**](https://docs.wandb.com/library/config) – Save all your hyperparameters in a configuration object so they can be logged. Read more about how to use `wandb.config` [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb).\n",
    "* [**`wandb.log()`**](https://docs.wandb.com/library/log) – log model behavior to W&B. Here, we just log the performance; see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb) for all the other rich media that can be logged with `wandb.log`.\n",
    "\n",
    "For more details on instrumenting W&B with PyTorch, see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMlizVTr5qMG"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "train_data_path = \"../data/preprocessing/01_pp_sg_train_cleaned.csv\"\n",
    "test_data_path = \"../data/preprocessing/01_pp_sg_test_cleaned.csv\"\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        model_path = os.path.join(wandb.run.dir, \"model_scripted.pt\")\n",
    "\n",
    "        X, y, input_dim, output_dim = load_features_labels(train_data_path)\n",
    "        X_train, X_val, y_train, y_val = split_train_val(X, y, config.train_val_ratio)\n",
    "        X_test, y_test, _, _ = load_features_labels(test_data_path)\n",
    "        X_scaler = apply_scaler(config.X_scaler)\n",
    "        y_scaler = apply_scaler(config.y_scaler)\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = X_scaler.scale(X_train, X_val, X_test)\n",
    "        y_train_scaled, y_val_scaled, y_test_scaled = y_scaler.scale(y_train, y_val, y_test)\n",
    "        train_loader = build_dataset(config.batch_size, X_train_scaled, y_train_scaled)\n",
    "        val_loader = build_dataset(config.batch_size, X_val_scaled, y_val_scaled)\n",
    "        test_loader = build_dataset(config.batch_size, X_test_scaled, y_test_scaled)\n",
    "        network = build_network(config.fc_layer_size, config.dropout, config.num_layers, input_dim, output_dim,\n",
    "                                config.model)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(network, train_loader, optimizer, config.batch_size, input_dim)\n",
    "            avg_val_loss = val_epoch(network, val_loader, config.batch_size, input_dim)\n",
    "            wandb.log({\"loss (train)\": avg_loss, \"epoch\": epoch})\n",
    "            wandb.log({\"loss (validation)\": avg_val_loss, \"epoch\": epoch})\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_epoch = epoch\n",
    "                print(f\"New best validation loss: {best_val_loss} at epoch {best_epoch}. Saving model to {model_path}.\")\n",
    "                save_model(network, model_path)\n",
    "\n",
    "        best_model = torch.jit.load(model_path, map_location=device)\n",
    "        avg_test_loss, test_outputs, test_targets = test_network(best_model, test_loader, config.batch_size, input_dim, y_scaler)\n",
    "        wandb.log({\"loss (test)\": avg_test_loss, \"epoch\": best_epoch})\n",
    "        plot_test_prediction(test_outputs, test_targets)\n",
    "        save_scaler(X_scaler, y_scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the data\n",
    "\n",
    "We load the data from the CSV files and split it into training and validation sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_features_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "\n",
    "    preprocess_features = PreprocessFeatures(df)\n",
    "\n",
    "    y = df[parking_data_labels]\n",
    "    X, input_dim = preprocess_features.get_features_for_model()\n",
    "\n",
    "    output_dim = len(y.columns)\n",
    "\n",
    "    print(f\"Input dimension: {input_dim}, columns: {X.columns}\")\n",
    "    print(f\"Output dimension: {output_dim}, columns: {y.columns}\")\n",
    "\n",
    "    return X, y, input_dim, output_dim\n",
    "\n",
    "\n",
    "def split_train_val(X, y, train_val_ratio):\n",
    "    train_size = int(len(X) * train_val_ratio)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "    return X_train, X_val, y_train, y_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply the scaler\n",
    "\n",
    "We apply the scaler to the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_scaler(scaler):\n",
    "    if scaler == \"standard\":\n",
    "        return Scaler(StandardScaler())\n",
    "    elif scaler == \"minmax\":\n",
    "        return Scaler(MinMaxScaler())\n",
    "    elif scaler == \"robust\":\n",
    "        return Scaler(RobustScaler())\n",
    "    elif scaler == \"maxabs\":\n",
    "        return Scaler(MaxAbsScaler())\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid scaler value: {scaler}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq8SQD9s5qMG"
   },
   "source": [
    "### Build the dataset\n",
    "\n",
    "We build the dataset from features (X) and labels (y). This is used for training data, validation data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_dataset(batch_size, X, y):\n",
    "    features = torch.Tensor(X)\n",
    "    targets = torch.Tensor(y)\n",
    "\n",
    "    dataset = TensorDataset(features, targets)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ],
   "metadata": {
    "id": "PMNNnYkr5qMG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the network\n",
    "\n",
    "We build the network with the given hyperparameters. We can choose between RNN, LSTM and GRU.\n",
    "\n",
    "We decided to use LSTM, because it is the most powerful of the three and we have a lot of data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_network(fc_layer_size, dropout, num_layers, input_dim, output_dim, model):\n",
    "    if model == \"rnn\":\n",
    "        network = RNNModel(input_dim=input_dim, hidden_dim=fc_layer_size, layer_dim=num_layers, output_dim=output_dim,\n",
    "                           dropout_prob=dropout)\n",
    "    elif model == \"lstm\":\n",
    "        network = LSTMModel(input_dim=input_dim, hidden_dim=fc_layer_size, layer_dim=num_layers, output_dim=output_dim,\n",
    "                            dropout_prob=dropout)\n",
    "    elif model == \"gru\":\n",
    "        network = GRUModel(input_dim=input_dim, hidden_dim=fc_layer_size, layer_dim=num_layers, output_dim=output_dim,\n",
    "                           dropout_prob=dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model value: {model}\")\n",
    "\n",
    "    return network.to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build the optimizer\n",
    "\n",
    "We build the optimizer with the given hyperparameters. We can choose between SGD and Adam."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the training and validation epochs, and the test network\n",
    "\n",
    "- `train_epoch` is used in every epoch to train the network. The training loss is calculated with the mean squared error.\n",
    "- `val_epoch` is used in every epoch to validate the network. The validation loss is calculated with the mean squared error.\n",
    "- `test_network` is used after the training to test the network. The test loss is calculated with the mean squared error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_epoch(network, loader, optimizer, batch_size, input_dim):\n",
    "    losses = []\n",
    "    network.train()\n",
    "    for _, (data, target) in enumerate(loader):\n",
    "        data, target = data.view([batch_size, -1, input_dim]).to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output = network(data.unsqueeze(0)).squeeze() # See https://medium.com/@mike.roweprediger/using-pytorch-to-train-an-lstm-forecasting-model-e5a04b6e0e67\n",
    "\n",
    "        # ➡ Forward pass\n",
    "        loss = F.mse_loss(network(data), target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # ⬅ Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"batch loss (train)\": loss.item()})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def val_epoch(network, loader, batch_size, input_dim):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        for _, (data, target) in enumerate(loader):\n",
    "            data, target = data.view([batch_size, -1, input_dim]).to(device), target.to(device)\n",
    "            loss = F.mse_loss(network(data), target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            wandb.log({\"batch loss (validation)\": loss.item()})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def test_network(network, loader, batch_size, input_dim, y_scaler):\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        for _, (data, target) in enumerate(loader):\n",
    "            data, target = data.view([batch_size, -1, input_dim]).to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            output_inverted = y_scaler.inverse_transform(output.detach().cpu().numpy())\n",
    "            target_inverted = y_scaler.inverse_transform(target.detach().cpu().numpy())\n",
    "            loss = mean_squared_error(output_inverted, target_inverted, squared=False)\n",
    "            losses.append(loss)\n",
    "            outputs.append(output_inverted)\n",
    "            targets.append(target_inverted)\n",
    "\n",
    "    return np.mean(losses), outputs, targets\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot the test prediction\n",
    "\n",
    "We plot the prediction and the target."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_test_prediction(outputs, targets):\n",
    "    for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "        if i % 10 != 0:\n",
    "            continue\n",
    "\n",
    "        df_output = pd.DataFrame(output, columns=parking_data_labels)\n",
    "        df_target = pd.DataFrame(target, columns=parking_data_labels)\n",
    "\n",
    "        n_features = len(df_output.columns)\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Setting the positions of the bars\n",
    "        ind = np.arange(n_features)  # the x locations for the groups\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        # Plotting bars for each row\n",
    "        bars1 = ax.bar(ind - width / 2, df_output.iloc[0], width, label='Prediction (from model)')\n",
    "        bars2 = ax.bar(ind + width / 2, df_target.iloc[0], width, label='Target (form dataset)')\n",
    "\n",
    "        # Adding some text for labels, title, and custom x-axis tick labels\n",
    "        ax.set_xlabel('Parking garages')\n",
    "        ax.set_ylabel('Free parking spots')\n",
    "        ax.set_title(f'Comparison of Two Rows in a Bar Chart {i}')\n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(df_output.columns)\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model and the scaler\n",
    "\n",
    "This can later be reused to predict real values. (see app.py)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_model(network, model_path):\n",
    "    model_scripted = torch.jit.script(network)\n",
    "    model_scripted.save(model_path)\n",
    "\n",
    "\n",
    "def save_scaler(X_scaler, y_scaler):\n",
    "    X_scaler_path = os.path.join(wandb.run.dir, \"X_scaler.pkl\")\n",
    "    y_scaler_path = os.path.join(wandb.run.dir, \"y_scaler.pkl\")\n",
    "    X_scaler.save(X_scaler_path)\n",
    "    y_scaler.save(y_scaler_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start the agent and run the sweep\n",
    "\n",
    "That someone is an `agent`, and they are created with `wandb.agent`.\n",
    "To get going, the agent just needs to know\n",
    "1. which Sweep it's a part of (`sweep_id`)\n",
    "2. which function it's supposed to run (here, `train`)\n",
    "3. (optionally) how many configs to ask the Controller for (`count`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjgClaFq5qMH"
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
