{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Bp7xn6t5qL_"
   },
   "source": [
    "\n",
    "# Hyperparameter Sweeps\n",
    "\n",
    "In this project, we use Hyperparemter sweeps with Pytorch on \"Weights & Biases\". For further details, check out this [Colab](http://wandb.me/sweeps-colab).\n",
    "\n",
    "Inspired by https://github.com/SheezaShabbir/Time-series-Analysis-using-LSTM-RNN-and-GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjckmLcx5qL_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Start out by installing the experiment tracking library and setting up your free W&B account:\n",
    "\n",
    "1. Install with `!pip install`\n",
    "2. `import` the library into Python\n",
    "3. `.login()` so you can log metrics to your projects\n",
    "\n",
    "If you've never used Weights & Biases before,\n",
    "the call to `login` will give you a link to sign up for an account.\n",
    "W&B is free to use for personal and academic projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install wandb -Uq"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiEyBrQm5qMB"
   },
   "source": [
    "## Defining the sweep config\n",
    "\n",
    "We define the sweep config via dict in our Jupyter notebook. You can find more information on sweeps in the [documentation](https://docs.wandb.com/sweeps/configuration).\n",
    "\n",
    "You can find a list of all configuration options [here](https://docs.wandb.com/library/sweeps/configuration) and a big collection of examples in YAML format [here](https://github.com/wandb/examples/tree/master/examples/keras/keras-cnn-fashion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilZtMXIA5qMC"
   },
   "outputs": [],
   "source": [
    "# See also https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'goal': 'minimize',\n",
    "        'name': 'loss'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'model': {\n",
    "            'values': ['lstm', 'rnn']\n",
    "        },\n",
    "        'scaler': {\n",
    "            'values': ['standard', 'minmax', 'robust', 'maxabs']\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'max': 256,\n",
    "            'min': 32,\n",
    "            'q': 8\n",
    "        },\n",
    "        'train_val_ratio': {\n",
    "            'value': 0.8\n",
    "        },\n",
    "        'dropout': {\n",
    "            'values': [0, 0.1, 0.2, 0.5]\n",
    "        },\n",
    "        'num_layers': {\n",
    "            'values': [2, 4, 8, 16, 32]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'values': [5, 10, 20, 40]\n",
    "        },\n",
    "        'fc_layer_size': {\n",
    "            'values': [50, 100, 200, 400, 1000]\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'uniform',\n",
    "            'max': 0.1,\n",
    "            'min': 0.00001\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd']\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHesSoz85qMF"
   },
   "source": [
    "## Initialize the setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1PbIZ_xm5qMG"
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"pp-sg-lstm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQH5jXxb5qMG"
   },
   "source": [
    "## Run the sweep agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkRbcMX35qMG"
   },
   "source": [
    "### Define Your Training Procedure\n",
    "\n",
    "Before we can actually execute the sweep, we need to define the training procedure that uses those values.\n",
    "\n",
    "In the functions below, we define a simple fully-connected neural network in PyTorch, and add the following `wandb` tools to log model metrics, visualize performance and output and track our experiments:\n",
    "* [**`wandb.init()`**](https://docs.wandb.com/library/init) â€“ Initialize a new W&B Run. Each Run is a single execution of the training function.\n",
    "* [**`wandb.config`**](https://docs.wandb.com/library/config) â€“ Save all your hyperparameters in a configuration object so they can be logged. Read more about how to use `wandb.config` [here](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-config/Configs_in_W%26B.ipynb).\n",
    "* [**`wandb.log()`**](https://docs.wandb.com/library/log) â€“ log model behavior to W&B. Here, we just log the performance; see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/wandb-log/Log_(Almost)_Anything_with_W%26B_Media.ipynb) for all the other rich media that can be logged with `wandb.log`.\n",
    "\n",
    "For more details on instrumenting W&B with PyTorch, see [this Colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/pytorch/Simple_PyTorch_Integration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UMlizVTr5qMG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, RobustScaler\n",
    "from models import LSTMModel, RNNModel, GRUModel\n",
    "from scaler import Scaler\n",
    "from data.metadata.metadata import feature_columns, parking_data_labels\n",
    "from data.preprocessing.preprocess_features import PreprocessFeatures\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Training on {device}\")\n",
    "\n",
    "train_data_path = \"../data/preprocessing/01_pp_sg_train_cleaned.csv\"\n",
    "test_data_path = \"../data/preprocessing/01_pp_sg_test_cleaned.csv\"\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        X, y, input_dim, output_dim = load_features_labels(train_data_path)\n",
    "        X_train, X_val, y_train, y_val = split_train_val(X, y, config.train_val_ratio)\n",
    "        X_test, y_test, _, _ = load_features_labels(test_data_path)\n",
    "        scaler = apply_scaler(config.scaler)\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled, y_train_scaled, y_val_scaled, y_test_scaled = scaler.scale(X_train,\n",
    "                                                                                                        X_val,\n",
    "                                                                                                        X_test,\n",
    "                                                                                                        y_train,\n",
    "                                                                                                        y_val,\n",
    "                                                                                                        y_test)\n",
    "        train_loader = build_dataset(config.batch_size, X_train_scaled, y_train_scaled)\n",
    "        val_loader = build_dataset(config.batch_size, X_val_scaled, y_val_scaled)\n",
    "        test_loader = build_dataset(config.batch_size, X_test_scaled, y_test_scaled)\n",
    "        network = build_network(config.fc_layer_size, config.dropout, config.num_layers, input_dim, output_dim,\n",
    "                                config.model)\n",
    "        optimizer = build_optimizer(network, config.optimizer, config.learning_rate)\n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "            avg_loss = train_epoch(network, train_loader, optimizer, config.batch_size, input_dim)\n",
    "            avg_val_loss = val_epoch(network, val_loader, config.batch_size, input_dim)\n",
    "            wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
    "            wandb.log({\"loss (validation)\": avg_val_loss, \"epoch\": epoch})\n",
    "\n",
    "        avg_test_loss, test_outputs, test_targets = test_network(network, test_loader, config.batch_size, input_dim)\n",
    "        wandb.log({\"loss (test)\": avg_test_loss})\n",
    "        plot_test_prediction(scaler, test_outputs, test_targets)\n",
    "        save_model_scaler(network, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_features_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path, sep=\";\")\n",
    "\n",
    "    preprocess_features = PreprocessFeatures(df)\n",
    "\n",
    "    # TODO unify this\n",
    "    # df['datetime'] = pd.to_datetime(df['datetime'], format='%d.%m.%Y %H:%M')\n",
    "\n",
    "    y = df[parking_data_labels]\n",
    "    X, input_dim = preprocess_features.get_features_for_model()\n",
    "\n",
    "    output_dim = len(y.columns)\n",
    "\n",
    "    print(f\"Input dimension: {input_dim}, columns: {X.columns}\")\n",
    "    print(f\"Output dimension: {output_dim}, columns: {y.columns}\")\n",
    "\n",
    "    return X, y, input_dim, output_dim\n",
    "\n",
    "\n",
    "def split_train_val(X, y, train_val_ratio):\n",
    "    train_size = int(len(X) * train_val_ratio)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "    return X_train, X_val, y_train, y_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_scaler(scaler):\n",
    "    if scaler == \"standard\":\n",
    "        return Scaler(StandardScaler())\n",
    "    elif scaler == \"minmax\":\n",
    "        return Scaler(MinMaxScaler())\n",
    "    elif scaler == \"robust\":\n",
    "        return Scaler(RobustScaler())\n",
    "    elif scaler == \"maxabs\":\n",
    "        return Scaler(MaxAbsScaler())\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid scaler value: {scaler}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq8SQD9s5qMG"
   },
   "source": [
    "This cell defines the four pieces of our training procedure:\n",
    "`build_dataset`, `build_network`, `build_optimizer`, and `train_epoch`.\n",
    "\n",
    "All of these are a standard part of a basic PyTorch pipeline,\n",
    "and their implementation is unaffected by the use of W&B,\n",
    "so we won't comment on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMNNnYkr5qMG"
   },
   "outputs": [],
   "source": [
    "def build_dataset(batch_size, X, y):\n",
    "    features = torch.Tensor(X)\n",
    "    targets = torch.Tensor(y)\n",
    "\n",
    "    dataset = TensorDataset(features, targets)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "def build_network(fc_layer_size, dropout, num_layers, input_dim, output_dim, model):\n",
    "    if model == \"rnn\":\n",
    "        network = RNNModel(input_dim=input_dim, hidden_dim=fc_layer_size, layer_dim=num_layers, output_dim=output_dim,\n",
    "                           dropout_prob=dropout)\n",
    "    elif model == \"lstm\":\n",
    "        network = LSTMModel(input_dim=input_dim, hidden_dim=fc_layer_size, layer_dim=num_layers, output_dim=output_dim,\n",
    "                            dropout_prob=dropout)\n",
    "    elif model == \"gru\":\n",
    "        network = GRUModel(input_dim=input_dim, hidden_dim=fc_layer_size, layer_dim=num_layers, output_dim=output_dim,\n",
    "                           dropout_prob=dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model value: {model}\")\n",
    "\n",
    "    return network.to(device)\n",
    "\n",
    "\n",
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_epoch(network, loader, optimizer, batch_size, input_dim):\n",
    "    losses = []\n",
    "    network.train()\n",
    "    for _, (data, target) in enumerate(loader):\n",
    "        data, target = data.view([batch_size, -1, input_dim]).to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output = network(data.unsqueeze(0)).squeeze() # See https://medium.com/@mike.roweprediger/using-pytorch-to-train-an-lstm-forecasting-model-e5a04b6e0e67\n",
    "\n",
    "        # âž¡ Forward pass\n",
    "        loss = F.mse_loss(network(data), target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # â¬… Backward pass + weight update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        wandb.log({\"batch loss\": loss.item()})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def val_epoch(network, loader, batch_size, input_dim):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        for _, (data, target) in enumerate(loader):\n",
    "            data, target = data.view([batch_size, -1, input_dim]).to(device), target.to(device)\n",
    "            loss = F.mse_loss(network(data), target)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            wandb.log({\"batch loss (validation)\": loss.item()})\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "\n",
    "def test_network(network, loader, batch_size, input_dim):\n",
    "    outputs = []\n",
    "    targets = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        for _, (data, target) in enumerate(loader):\n",
    "            data, target = data.view([batch_size, -1, input_dim]).to(device), target.to(device)\n",
    "            output = network(data)\n",
    "            loss = F.mse_loss(output, target)\n",
    "            outputs.append(output.detach().cpu().numpy())\n",
    "            targets.append(target.detach().cpu().numpy())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return np.mean(losses), outputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def inverse_transform(scaler, df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = scaler.inverse_transform(df[col])\n",
    "    return df\n",
    "\n",
    "def plot_test_prediction(scaler, outputs, targets):\n",
    "    outputs = inverse_transform(scaler, pd.DataFrame(np.concatenate(outputs)), parking_data_labels)\n",
    "    targets = inverse_transform(scaler, pd.DataFrame(np.concatenate(targets)), parking_data_labels)\n",
    "\n",
    "    for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "        if i % 10 != 0:\n",
    "            continue\n",
    "\n",
    "        df_output = pd.DataFrame(output, columns=parking_data_labels)\n",
    "        df_target = pd.DataFrame(target, columns=parking_data_labels)\n",
    "\n",
    "        n_features = len(df_output.columns)\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        # Setting the positions of the bars\n",
    "        ind = np.arange(n_features)  # the x locations for the groups\n",
    "        width = 0.35  # the width of the bars\n",
    "\n",
    "        # Plotting bars for each row\n",
    "        bars1 = ax.bar(ind - width / 2, df_output.iloc[0], width, label='Prediction (from model)')\n",
    "        bars2 = ax.bar(ind + width / 2, df_target.iloc[0], width, label='Target (form dataset)')\n",
    "\n",
    "        # Adding some text for labels, title, and custom x-axis tick labels\n",
    "        ax.set_xlabel('Parking garages')\n",
    "        ax.set_ylabel('Free parking spots')\n",
    "        ax.set_title(f'Comparison of Two Rows in a Bar Chart {i}')\n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(df_output.columns)\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_model_scaler(network, scaler):\n",
    "    model_scripted = torch.jit.script(network)\n",
    "    model_path = os.path.join(wandb.run.dir, \"model_scripted.pt\")\n",
    "    print(f\"Saving model to {model_path}\")\n",
    "    model_scripted.save(model_path)\n",
    "    scaler.save(os.path.join(wandb.run.dir, \"scaler.pkl\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIEd9-Gm5qMH"
   },
   "source": [
    "The cell below will launch an `agent` that runs `train` 5 times,\n",
    "usingly the randomly-generated hyperparameter values returned by the Sweep Controller. Execution takes under 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we're ready to start sweeping! ðŸ§¹ðŸ§¹ðŸ§¹\n",
    "\n",
    "Sweep Controllers, like the one we made by running `wandb.sweep`, sit waiting for someone to ask them for a `config` to try out.\n",
    "\n",
    "That someone is an `agent`, and they are created with `wandb.agent`.\n",
    "To get going, the agent just needs to know\n",
    "1. which Sweep it's a part of (`sweep_id`)\n",
    "2. which function it's supposed to run (here, `train`)\n",
    "3. (optionally) how many configs to ask the Controller for (`count`)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjgClaFq5qMH"
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
